{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "lastEditStatus": {
   "notebookId": "wexdiba2y4g2lehxkfwc",
   "authorId": "5713202290",
   "authorName": "TIMBURNSOWLMTN1",
   "authorEmail": "timburnsowlmtn@gmail.com",
   "sessionId": "365bb9da-712e-4148-ab39-a327f50422ec",
   "lastEditTime": 1746906036015
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "name": "cell1",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n\n# Import python packages\nimport streamlit as st\nimport pandas as pd  #%%\n# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n\n\n     ",
   "id": "7a23888c-09cf-4a82-a3cf-091e014e72dd"
  },
  {
   "cell_type": "code",
   "id": "67cb57ac-95cd-46e9-8efd-ac28fc9a5e73",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "\nsales = session.table(\"SALES\").to_pandas()\npurchase = session.table(\"PURCHASES\").to_pandas()\ninvoice = session.table(\"INVOICE_PURCHASE\").to_pandas()\nend_inv = session.table(\"END_INV\").to_pandas()\nbeg_inv = session.table(\"BEG_INV\").to_pandas()\ndisplay(beg_inv.head())\ndisplay(end_inv.head())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e11e8f1-68d2-4939-975f-65301965ce4c",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "\nnuniques = {\"beg_inv\": beg_inv.nunique(), \"end_inv\": end_inv.nunique()}\ndisplay(\n    pd.DataFrame(nuniques).T[\n        [\n            \"INVENTORYID\",\n            \"STORE\",\n            \"CITY\",\n            \"BRAND\",\n            \"DESCRIPTION\",\n            \"SIZE\",\n            \"ONHAND\",\n            \"BEGINDATE\",\n            \"ENDDATE\",\n        ]\n    ]\n)\nprint(\n    f\"BEG_INV BRAND nunique: {beg_inv.BRAND.nunique()}, desc + size nunique: {(beg_inv['DESCRIPTION'] + ' ' + beg_inv['SIZE']).nunique()}??? Might need cleaning\"\n)\nprint(\n    f\"end_inv Brand nunique: {end_inv.BRAND.nunique()}, desc + size nunique: {(end_inv['DESCRIPTION'] + ' ' + end_inv['SIZE']).nunique()}\"\n)\nbeg_inv_brand = beg_inv.loc[:]\nbeg_inv_brand[\"DESC_SIZE\"] = beg_inv_brand[\"DESCRIPTION\"] + \" \" + beg_inv_brand[\"SIZE\"]\ngroup_desc = (\n    beg_inv_brand[[\"BRAND\", \"DESC_SIZE\"]].groupby(\"DESC_SIZE\")[\"BRAND\"].unique()\n)\ngroup_desc.loc[group_desc.apply(len) > 1]\n\ndisplay(sales.head())\ndisplay(\n    beg_inv.loc[\n        (beg_inv[\"BRAND\"] == 1004) & (beg_inv[\"INVENTORYID\"] == \"1_HARDERSFIELD_1004\")\n        ]\n)\n# ? Inventory ID = store_city_brand, Brand = description + Size, With Inventory ID we can find how many onhand the inventory have at the beginning and end.\nprint(sales.CLASSIFICATION.unique())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29eca113-4c91-46c1-bf68-44ca0c8fc962",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## 1. Aggregate the data from the tables\n\n- **Group** the `sales` DataFrame by the `\"SALESDATE\"` column.  \n- **Aggregate** by summing `\"SALESQUANTITY\"` to get total daily sales.  \n- Result stored in a new DataFrame `sales_quantity_price`.\n\n"
  },
  {
   "cell_type": "code",
   "id": "0989e51c-30ca-4c49-9f68-06a2873a3f2f",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# * group by date, sum sales quantity to get total sales quantity per day\nsales_quantity_price = sales.groupby(\"SALES_DATE\").agg({\"SALES_QUANTITY\": \"sum\"})\n\nsales_quantity_price.describe();\n\nprint(sales_quantity_price);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5098f37-d49d-4c1f-8451-190a69d21554",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "\n\n## 2. Native Snowflake ML Forecasting\n\nSnowflake provides built-in time-series forecasting as a SQL object you train and invoke entirely in SQL"
  },
  {
   "cell_type": "code",
   "id": "88441f5b-8760-46e0-840c-5e46d600f91a",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "df_to_write = sales_quantity_price.reset_index()\n\n\nsnowpark_df = session.create_dataframe(df_to_write)  \nsnowpark_df.write.mode(\"overwrite\").save_as_table(\"SALES_QUANTITY_PRICE\");\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99d651f9-cf69-4b1c-83ee-c87d456f153a",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "-- 1) Train the model (requires CREATE SNOWFLAKE.ML.FORECAST privilege)\nCREATE SNOWFLAKE.ML.FORECAST inventory_forecast_model (\n  INPUT_DATA       => TABLE(sales_quantity_price),\n  TIMESTAMP_COLNAME=> 'SALES_DATE',\n  TARGET_COLNAME   => 'SALES_QUANTITY'\n);\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15fe47c8-bb69-4239-925d-465f7136da9c",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "-- 2) Generate a 90-day forecast\nCREATE or REPLACE table sales_quantity_price_forecast AS\nSELECT * \nFROM TABLE(inventory_forecast_model!FORECAST(FORECASTING_PERIODS => 90));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "765e9257-4cf4-43be-8c20-a04227b0a915",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# â”€â”€ 2) Load your tables into pandas (using SALES_DATE & SALES_QUANTITY) â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsales_df = (\n    session\n    .table(\"SALES\")\n    .select(\"SALES_DATE\", \"SALES_QUANTITY\")\n    .to_pandas()\n)\nsq_df = (\n    session\n    .table(\"SALES_QUANTITY_PRICE\")\n    .select(\"SALES_DATE\", \"SALES_QUANTITY\")\n    .to_pandas()\n)\n\n# Ensure the date columns are datetime\nsales_df[\"SALES_DATE\"] = pd.to_datetime(sales_df[\"SALES_DATE\"])\nsq_df[\"SALES_DATE\"]    = pd.to_datetime(sq_df[\"SALES_DATE\"])\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb22e1ae-d866-43f2-af38-c618d127b4eb",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n# â”€â”€ 3) Merge on SALES_DATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmerged = pd.merge(\n    sales_df.rename(columns={\"SALES_QUANTITY\": \"Raw_Sales\"}),\n    sq_df.rename(columns={\"SALES_QUANTITY\": \"Agg_Sales\"}),\n    on=\"SALES_DATE\",\n    how=\"inner\"\n)\n# Rename for plotting\nmerged = merged.rename(columns={\"SALES_DATE\": \"Date\"}).set_index(\"Date\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a854fa1-177a-4c6d-bb85-d8051afccfd8",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "### Summary\n\n- **Built-in FORECAST** only returns future forecasts, not fitted training values :contentReference[oaicite:1]{index=1}.  \n- To get in-sample predictions you must either **wrap** an external ARIMA implementation in a Python UDF or run your ARIMA entirely in a Container Runtime notebook and persist the `fittedvalues` yourself.  \n- Snowflake does not currently expose a table function for fitted (in-sample) values from `SNOWFLAKE.ML.FORECAST`.  "
  },
  {
   "cell_type": "code",
   "id": "10884a6e-5f2b-4b88-9bd8-4b3942e7c575",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "\n\n# 4) Render with Streamlit\nst.title(\"ðŸ“ˆ Raw vs Aggregated Sales Quantity\")\nst.line_chart(merged.iloc[::32])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55e77c40-4cb4-4821-ba40-fdebbc830d44",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "# Load historical and forecast tables into Pandas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  },
  {
   "cell_type": "code",
   "id": "bea5200d-451f-41a0-93ca-def93588d088",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "# â”€â”€ 4) Build visualization DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Focus on forecast and its bounds\nviz_df = (\n    merged\n    .set_index(\"SALES_DATE\")\n    .sort_index()[[\"SALES_QUANTITY\", \"LOWER_BOUND\", \"UPPER_BOUND\"]]\n)\n\n# â”€â”€ 5) Render in Streamlit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nst.title(\"ðŸ•’ Sales Quantity Forecast with Prediction Intervals\")\nst.line_chart(viz_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddf171d8-c5ad-4205-985f-121a93bce8ee",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "\nsp_df = (\n    session\n    .table(\"SALES_QUANTITY_PRICE\")\n    .select(\"SALES_DATE\", \"SALES_QUANTITY\")\n    .to_pandas()\n)\n\n\nspf_df = (\n    session\n    .table(\"SALES_QUANTITY_PRICE_FORECAST\")\n    .select(\"TS\", \"FORECAST\", \"LOWER_BOUND\", \"UPPER_BOUND\")\n    .to_pandas()\n)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b2876b1-af7a-4472-a66d-5c6524f1bdd7",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "# â”€â”€ Prepare DataFrames â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1) Parse the historical SALES_DATE column\nsp_df[\"SALES_DATE\"] = pd.to_datetime(sp_df[\"SALES_DATE\"])\n\n# 2) Parse the forecast SALES_DATE column\nspf_df[\"SALES_DATE\"] = pd.to_datetime(spf_df[\"SALES_DATE\"])\n\n# 3) Merge on SALES_DATE (outer to include all dates)\nmerged = pd.merge(\n    sp_df,\n    spf_df,\n    on=\"SALES_DATE\",\n    how=\"outer\"\n)\n\n# 4) For forecast periods, replace missing SALES_QUANTITY with the forecast value\nmerged[\"SALES_QUANTITY\"] = merged[\"SALES_QUANTITY\"].fillna(merged[\"FORECAST\"])\n\n# 5) (Optional) Drop the now-redundant FORECAST column\nmerged = merged.drop(columns=[\"FORECAST\"])\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2745821c-7429-4840-9e7f-c222f95904f1",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "## We want to integrate the process into Cortex to see the power of DATA + ML + LLM"
  },
  {
   "cell_type": "code",
   "id": "9afe3156-35e7-4440-940f-992c969cf3dd",
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": "CALL SNOWFLAKE.MODELS.CORTEX_BASE_MODELS_REFRESH();\nSHOW MODELS IN SNOWFLAKE.MODELS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5489cd83-7c9e-4612-8d0a-8a336bda34f4",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "sp_forecast_df = session.create_dataframe(merged)  \nsp_forecast_df.write.mode(\"overwrite\").save_as_table(\"SALES_FORECAST_FULL\");",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03ddeff9-cbe2-4334-8098-de4143061be6",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": "CREATE or REPLACE TABLE CORTEX_OUT AS\nSELECT\n  SNOWFLAKE.CORTEX.COMPLETE(\n    'LLAMA3-8B',\n    $$  \n    I have a table SALES_FORECAST_FULL with columns:\n      â€¢ SALES_DATE  \n      â€¢ SALES_QUANTITY (actual or imputed forecast)  \n      â€¢ FORECAST  \n      â€¢ LOWER_BOUND  \n      â€¢ UPPER_BOUND  \n      \n    Please summarize the key trends in the forecast, comment on how the model performed during the historical period, and highlight any notable patterns or anomalies in the 90-day forecast.  \n    $$\n  ) AS analysis;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d823f64c-0b98-40a0-8816-8ccde5c18370",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": "SELECT * FROM CORTEX_OUT",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "787b6475-6812-42a1-86a3-cc1d66ccaeed",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "Based on the provided table, I'll summarize the key trends in the forecast, comment on the model's performance during the historical period, and highlight any notable patterns or anomalies in the 90-day forecast.\n\n**Key Trends:**\n\n1. **Trend in Sales Quantity:** The trend in sales quantity over time can be analyzed by plotting the actual or imputed forecast values against the sales date. This will help identify any upward or downward trends, seasonality, or anomalies.\n2. **Forecast Accuracy:** The mean absolute error (MAE) or mean absolute percentage error (MAPE) can be calculated to evaluate the model's performance in predicting sales quantity. A lower MAE or MAPE indicates better forecast accuracy.\n3. **Confidence Intervals:** The lower and upper bounds can be used to calculate the confidence intervals for the forecast. This will provide a range of possible values for the sales quantity, giving an idea of the uncertainty associated with the forecast.\n\n**Model Performance during Historical Period:**\n\n1. **Forecast Accuracy:** Analyze the MAE or MAPE for the historical period to evaluate the model's performance. A lower MAE or MAPE indicates better forecast accuracy.\n2. **Trend in Forecast Errors:** Plot the forecast errors (actual - forecast) over time to identify any patterns or trends in the errors. This can help identify areas where the model is consistently under- or over-forecasting.\n3. **Seasonality:** Examine the sales data and forecast for seasonality, which can be identified by plotting the data over time and looking for repeating patterns.\n\n**Notable Patterns or Anomalies in 90-day Forecast:**\n\n1. **Anomalies:** Identify any unusual or unexpected patterns in the 90-day forecast, such as sudden changes in sales quantity or unusual spikes in demand.\n2. **Trends:** Analyze the 90-day forecast for any emerging trends or patterns that may not be present in the historical data.\n3. **Confidence Intervals:** Use the lower and upper bounds to calculate the confidence intervals for the 90-day forecast. This will provide a range of possible values for the sales quantity, giving an idea of the uncertainty associated with the forecast.\n\nTo perform these analyses, you can use statistical software such as R or Python, or data visualization tools like Tableau or Power BI. The specific steps and techniques will depend on the complexity of your data and the insights you're trying to gain.\n\nHere's a sample R code to get you started:\n```R\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(forecast)\n\n# Load the data\ndata <- read.csv(\"SALES_FORECAST_FULL.csv\")\n\n# Plot the sales quantity over time\nggplot(data, aes(x = SALES_DATE, y = SALES_QUANTITY)) + \n  geom_line() + \n  theme_classic()\n\n# Calculate the mean absolute error (MAE)\nmae <- mean(abs(data$SALES_QUANTITY - data$FORECAST))\n\n# Calculate the mean absolute percentage error (MAPE)\nmape <- mean(abs((data$SALES_QUANTITY - data$FORECAST) / data$SALES_QUANTITY)) * 100\n\n# Plot the forecast errors over time\nggplot(data, aes(x = SALES_DATE, y = actual - forecast)) + \n  geom_line() + \n  theme_classic()\n\n# Calculate the confidence intervals for the 90-day forecast\nforecast_90d <- forecast(data, h = 90)\nconf_int <- forecast_90d$conf.int\n\n# Plot the 90-day forecast with confidence intervals\nggplot(forecast_90d, aes(x = time, y = fit)) + \n  geom_line() + \n  geom_ribbon(aes(ymin = conf_int[, 1], ymax = conf_int[, 2]), alpha = 0.2) + \n  theme_classic()\n```\nThis code provides a basic example of how to load the data, plot the sales quantity over time, calculate the MAE and MAPE, plot the forecast errors, and calculate the confidence intervals for the 90-day forecast. You can modify and extend this code to suit your specific needs and gain deeper insights into your sales forecast data."
  }
 ]
}