{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "lastEditStatus": {
   "notebookId": "wexdiba2y4g2lehxkfwc",
   "authorId": "5713202290",
   "authorName": "TIMBURNSOWLMTN1",
   "authorEmail": "timburnsowlmtn@gmail.com",
   "sessionId": "a7b7a531-c25c-4192-9836-9b6a1c0923ff",
   "lastEditTime": 1746910766199
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "name": "cell1",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n\n# Import python packages\nimport streamlit as st\nimport pandas as pd  #%%\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n\n\n     ",
   "id": "7a23888c-09cf-4a82-a3cf-091e014e72dd"
  },
  {
   "cell_type": "code",
   "id": "67cb57ac-95cd-46e9-8efd-ac28fc9a5e73",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "\nsales = session.table(\"SALES\").to_pandas()\npurchase = session.table(\"PURCHASES\").to_pandas()\ninvoice = session.table(\"INVOICE_PURCHASE\").to_pandas()\nend_inv = session.table(\"END_INV\").to_pandas()\nbeg_inv = session.table(\"BEG_INV\").to_pandas()\ndisplay(beg_inv.head())\ndisplay(end_inv.head())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e11e8f1-68d2-4939-975f-65301965ce4c",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "\nnuniques = {\"beg_inv\": beg_inv.nunique(), \"end_inv\": end_inv.nunique()}\ndisplay(\n    pd.DataFrame(nuniques).T[\n        [\n            \"INVENTORYID\",\n            \"STORE\",\n            \"CITY\",\n            \"BRAND\",\n            \"DESCRIPTION\",\n            \"SIZE\",\n            \"ONHAND\",\n            \"BEGINDATE\",\n            \"ENDDATE\",\n        ]\n    ]\n)\nprint(\n    f\"BEG_INV BRAND nunique: {beg_inv.BRAND.nunique()}, desc + size nunique: {(beg_inv['DESCRIPTION'] + ' ' + beg_inv['SIZE']).nunique()}??? Might need cleaning\"\n)\nprint(\n    f\"end_inv Brand nunique: {end_inv.BRAND.nunique()}, desc + size nunique: {(end_inv['DESCRIPTION'] + ' ' + end_inv['SIZE']).nunique()}\"\n)\nbeg_inv_brand = beg_inv.loc[:]\nbeg_inv_brand[\"DESC_SIZE\"] = beg_inv_brand[\"DESCRIPTION\"] + \" \" + beg_inv_brand[\"SIZE\"]\ngroup_desc = (\n    beg_inv_brand[[\"BRAND\", \"DESC_SIZE\"]].groupby(\"DESC_SIZE\")[\"BRAND\"].unique()\n)\ngroup_desc.loc[group_desc.apply(len) > 1]\n\ndisplay(sales.head())\ndisplay(\n    beg_inv.loc[\n        (beg_inv[\"BRAND\"] == 1004) & (beg_inv[\"INVENTORYID\"] == \"1_HARDERSFIELD_1004\")\n        ]\n)\n# ? Inventory ID = store_city_brand, Brand = description + Size, With Inventory ID we can find how many onhand the inventory have at the beginning and end.\nprint(sales.CLASSIFICATION.unique())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29eca113-4c91-46c1-bf68-44ca0c8fc962",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## 1. Aggregate the data from the tables\n\n- **Group** the `sales` DataFrame by the `\"SALESDATE\"` column.  \n- **Aggregate** by summing `\"SALESQUANTITY\"` to get total daily sales.  \n- Result stored in a new DataFrame `sales_quantity_price`.\n\n"
  },
  {
   "cell_type": "code",
   "id": "0989e51c-30ca-4c49-9f68-06a2873a3f2f",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# * group by date, sum sales quantity to get total sales quantity per day\nsales_quantity_price = sales.groupby(\"SALES_DATE\").agg({\"SALES_QUANTITY\": \"sum\"})\n\nsales_quantity_price.describe();\n\nprint(sales_quantity_price);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5098f37-d49d-4c1f-8451-190a69d21554",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "\n\n## 2. Native Snowflake ML Forecasting\n\nSnowflake provides built-in time-series forecasting as a SQL object you train and invoke entirely in SQL"
  },
  {
   "cell_type": "code",
   "id": "88441f5b-8760-46e0-840c-5e46d600f91a",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "df_to_write = sales_quantity_price.reset_index()\n\n\nsnowpark_df = session.create_dataframe(df_to_write)  \nsnowpark_df.write.mode(\"overwrite\").save_as_table(\"SALES_QUANTITY_PRICE\");\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99d651f9-cf69-4b1c-83ee-c87d456f153a",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "-- 1) Train the model (requires CREATE SNOWFLAKE.ML.FORECAST privilege)\nCREATE or REPLACE SNOWFLAKE.ML.FORECAST inventory_forecast_model (\n  INPUT_DATA       => TABLE(sales_quantity_price),\n  TIMESTAMP_COLNAME=> 'SALES_DATE',\n  TARGET_COLNAME   => 'SALES_QUANTITY'\n);\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15fe47c8-bb69-4239-925d-465f7136da9c",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "-- 2) Generate a forecast\nCREATE or REPLACE table sales_quantity_price_forecast AS\nSELECT * \nFROM TABLE(inventory_forecast_model!FORECAST(FORECASTING_PERIODS => 30));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4737572f-7a86-4181-bd4d-3c9889d48a60",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": "print()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "765e9257-4cf4-43be-8c20-a04227b0a915",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# â”€â”€ 2) Load your tables into pandas (using SALES_DATE & SALES_QUANTITY) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nsq_df = (\n    session\n    .table(\"SALES_QUANTITY_PRICE\")\n    .select(\"SALES_DATE\", \"SALES_QUANTITY\")\n    .to_pandas()\n)\n\n# Ensure the date columns are datetime\nsq_df[\"SALES_DATE\"] = pd.to_datetime(sales_df[\"SALES_DATE\"])\nsq_df.set_index(\"SALES_DATE\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a854fa1-177a-4c6d-bb85-d8051afccfd8",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "### Summary\n\n- **Built-in FORECAST** only returns future forecasts, not fitted training values :contentReference[oaicite:1]{index=1}.  \n- To get in-sample predictions you must either **wrap** an external ARIMA implementation in a Python UDF or run your ARIMA entirely in a Container Runtime notebook and persist the `fittedvalues` yourself.  \n- Snowflake does not currently expose a table function for fitted (in-sample) values from `SNOWFLAKE.ML.FORECAST`.  "
  },
  {
   "cell_type": "code",
   "id": "10884a6e-5f2b-4b88-9bd8-4b3942e7c575",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "sq_df = (\n    session\n    .table(\"SALES_QUANTITY_PRICE\")\n    .select(\"SALES_DATE\", \"SALES_QUANTITY\")\n    .to_pandas()\n)\n\n# 3) Prepare the DataFrame\nsq_df[\"SALES_DATE\"] = pd.to_datetime(sq_df[\"SALES_DATE\"])\nsq_df = sq_df.set_index(\"SALES_DATE\").sort_index()\n\n# 4) Render with Streamlit\nst.title(\"ðŸ“ˆ Raw vs Aggregated Sales Quantity\")\nst.line_chart(sq_df[\"SALES_QUANTITY\"])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55e77c40-4cb4-4821-ba40-fdebbc830d44",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "# Load historical and forecast tables into Pandas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  },
  {
   "cell_type": "code",
   "id": "ddf171d8-c5ad-4205-985f-121a93bce8ee",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# 1) Load your historical sales\nsp_df = (\n    session\n    .table(\"SALES_QUANTITY_PRICE\")\n    .select(\"SALES_DATE\", \"SALES_QUANTITY\")\n    .to_pandas()\n)\n# ensure SALES_DATE is datetime\nsp_df[\"SALES_DATE\"] = pd.to_datetime(sp_df[\"SALES_DATE\"])\n\n# 2) Load your forecast data and rename TS â†’ SALES_DATE\nspf_df = (\n    session\n    .table(\"SALES_QUANTITY_PRICE_FORECAST\")\n    .select(\"TS\", \"FORECAST\", \"LOWER_BOUND\", \"UPPER_BOUND\")\n    .to_pandas()\n)\nspf_df.rename(columns={\"TS\": \"SALES_DATE\"}, inplace=True)\nspf_df[\"SALES_DATE\"] = pd.to_datetime(spf_df[\"SALES_DATE\"])\n\n# 3) Merge on SALES_DATE (outer join to include all dates)\nmerged_df = pd.merge(\n    sp_df,\n    spf_df,\n    on=\"SALES_DATE\",\n    how=\"outer\"\n).sort_values(\"SALES_DATE\")\n\n# 4) (Optional) set the date as index\nmerged_df.set_index(\"SALES_DATE\", inplace=True)\n\n# merged_df now contains:\n#  - SALES_QUANTITY (historical)\n#  - FORECAST, LOWER_BOUND, UPPER_BOUND (forecasted)\nprint(merged_df.head())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17265718-1c60-44c2-ba94-40e8ae5fbbc8",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# Visualize in Streamlit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nst.title(\"ðŸ“Š Actual vs Forecast Sales Quantity with Confidence Bounds\")\n\n# Show the raw merged table\nst.subheader(\"Merged Data Table\")\nst.dataframe(merged_df)\n\n# Plot the time series: actual, forecast, lower & upper bounds\nst.subheader(\"Time Series Plot\")\nst.line_chart(\n    merged_df[[\"SALES_QUANTITY\", \"FORECAST\", \"LOWER_BOUND\", \"UPPER_BOUND\"]],\n    height=400,\n    use_container_width=True\n)\n\n\n\n\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2745821c-7429-4840-9e7f-c222f95904f1",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "## We want to integrate the process into Cortex to see the power of DATA + ML + LLM"
  },
  {
   "cell_type": "code",
   "id": "9afe3156-35e7-4440-940f-992c969cf3dd",
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": "-- This doesn't need to run every time.  CALL SNOWFLAKE.MODELS.CORTEX_BASE_MODELS_REFRESH();\nSHOW MODELS IN SNOWFLAKE.MODELS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5489cd83-7c9e-4612-8d0a-8a336bda34f4",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "sp_forecast_df = session.create_dataframe(merged_df)  \nsp_forecast_df.write.mode(\"overwrite\").save_as_table(\"SALES_FORECAST_FULL\");",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03ddeff9-cbe2-4334-8098-de4143061be6",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": "CREATE or REPLACE TABLE CORTEX_OUT AS\nSELECT\n  SNOWFLAKE.CORTEX.COMPLETE(\n    'LLAMA3-8B',\n    $$  \n    I have a table SALES_FORECAST_FULL with columns:\n      â€¢ SALES_DATE  \n      â€¢ SALES_QUANTITY (actual or imputed forecast)  \n      â€¢ FORECAST  \n      â€¢ LOWER_BOUND  \n      â€¢ UPPER_BOUND  \n      \n    Please summarize the key trends in the forecast, comment on how the model performed during the historical period, and highlight any notable patterns or anomalies in the 30-day forecast.  \n    $$\n  ) AS analysis;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d823f64c-0b98-40a0-8816-8ccde5c18370",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": "SELECT * FROM CORTEX_OUT",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "787b6475-6812-42a1-86a3-cc1d66ccaeed",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "Based on the provided table, I'll summarize the key trends in the forecast, comment on the model's performance during the historical period, and highlight any notable patterns or anomalies in the 30-day forecast.\n\n**Key Trends:**\n\n1. **Trend in Sales Quantity:** The trend in sales quantity over time can be analyzed by plotting the actual sales quantity against the forecasted sales quantity. This will help identify if the model is capturing the underlying trend in sales.\n2. **Seasonality:** The presence of seasonality can be checked by analyzing the sales quantity and forecasted sales quantity over different days of the week, months, or quarters. This will help identify if the model is capturing seasonal patterns.\n3. **Day-of-the-Week Effect:** The day-of-the-week effect can be analyzed by plotting the sales quantity and forecasted sales quantity for each day of the week. This will help identify if the model is capturing the typical patterns of sales on different days of the week.\n4. **Month-of-the-Year Effect:** The month-of-the-year effect can be analyzed by plotting the sales quantity and forecasted sales quantity for each month of the year. This will help identify if the model is capturing the typical patterns of sales during different months of the year.\n\n**Model Performance:**\n\n1. **Mean Absolute Error (MAE):** The MAE can be calculated to measure the average difference between the actual sales quantity and the forecasted sales quantity. A lower MAE indicates better model performance.\n2. **Mean Squared Error (MSE):** The MSE can be calculated to measure the average squared difference between the actual sales quantity and the forecasted sales quantity. A lower MSE indicates better model performance.\n3. **Root Mean Squared Percentage Error (RMSPE):** The RMSPE can be calculated to measure the average percentage difference between the actual sales quantity and the forecasted sales quantity. A lower RMSPE indicates better model performance.\n\n**Notable Patterns or Anomalies in the 30-day Forecast:**\n\n1. **Outliers:** Any extreme values in the 30-day forecast can be identified and investigated to determine if they are anomalies or if they indicate a change in the underlying trend.\n2. **Trend Breaks:** Any changes in the trend of the sales quantity or forecasted sales quantity can be identified and investigated to determine if they are anomalies or if they indicate a change in the underlying trend.\n3. **Seasonal Patterns:** Any changes in the seasonal patterns of the sales quantity or forecasted sales quantity can be identified and investigated to determine if they are anomalies or if they indicate a change in the underlying trend.\n\nTo perform these analyses, you can use various statistical and data visualization techniques, such as:\n\n1. Time series plots to visualize the trend and seasonality in the data.\n2. Scatter plots to visualize the relationship between the actual sales quantity and the forecasted sales quantity.\n3. Box plots to visualize the distribution of the errors between the actual sales quantity and the forecasted sales quantity.\n4. Regression analysis to identify the relationship between the actual sales quantity and the forecasted sales quantity.\n5. Statistical tests, such as the Augmented Dickey-Fuller test, to identify the presence of seasonality and trend breaks.\n\nBy performing these analyses, you can gain insights into the key trends in the forecast, the model's performance during the historical period, and any notable patterns or anomalies in the 30-day forecast."
  }
 ]
}